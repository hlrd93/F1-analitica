# PresentaciÃ³n del Proyecto - F1 Analytics Pipeline

**Estudiante:** Herwin Leonardo Rey Diaz 
**Programa:** MaestrÃ­a en Ciencia de Datos - Universidad CatÃ³lica del Uruguay (UCU)  
**Fecha:** Diciembre 2025

---

## ğŸ“– DocumentaciÃ³n Completa con MkDocs

> **âš ï¸ IMPORTANTE:** Para visualizar la **documentaciÃ³n completa del proyecto**, incluyendo arquitectura, metodologÃ­a, diagramas y decisiones tÃ©cnicas, debe levantar el servidor MkDocs.

### CÃ³mo Acceder a la DocumentaciÃ³n

```bash
# 1. Instalar MkDocs (si no estÃ¡ instalado)
pip install mkdocs-material

# 2. Compilar la documentaciÃ³n
mkdocs build

# 3. Levantar el servidor local
mkdocs serve

# 4. Abrir en el navegador
# La documentaciÃ³n estarÃ¡ disponible en: http://localhost:8000
```

La documentaciÃ³n en MkDocs incluye:
- ğŸ—ï¸ **Arquitectura** - Diagramas del pipeline y componentes
- ğŸ“Š **MetodologÃ­a** - Enfoque y decisiones metodolÃ³gicas
- ğŸ”§ **ImplementaciÃ³n** - GuÃ­as tÃ©cnicas detalladas
- ğŸ’‰ **Ingesta de datos** - Proceso ETL/ELT
- ğŸ“¸ **Snapshots** - SCD Type 2
- ğŸ¯ **Casos de uso** - Ejemplos de anÃ¡lisis
- ğŸ“‹ **ADR** - Architecture Decision Records

---

## ğŸ“ UbicaciÃ³n del Proyecto

### Repositorio GitHub
- **URL:** https://github.com/hlrd93/F1-analitica
- **Branch Principal:** `main`
- **Visibilidad:** PÃºblico

### DocumentaciÃ³n en LÃ­nea
- **Sitio MkDocs:** https://hlrd93.github.io/F1-analitica/ 
- **DocumentaciÃ³n local:** Disponible en el directorio `/docs`

---

## ğŸ¯ DescripciÃ³n del Proyecto

Pipeline de anÃ¡lisis de datos de FÃ³rmula 1 construido con arquitectura ELT moderna, utilizando ClickHouse como motor OLAP columnar, dbt para transformaciones SQL, y Streamlit para visualizaciones interactivas.

### Objetivo Principal
Implementar un pipeline de datos end-to-end que cumpla con los requisitos acadÃ©micos de la maestrÃ­a UCU, demostrando:
- Modelado dimensional (Star Schema)
- Procesos ETL/ELT automatizados
- Calidad de datos con tests automatizados
- VisualizaciÃ³n mediante dashboards interactivos
- DocumentaciÃ³n tÃ©cnica completa

---

## ğŸ—ï¸ Arquitectura TÃ©cnica

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CSV Files  â”‚
â”‚  (datasets/)â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Python Ingestion   â”‚
â”‚  load_csvs_to_ch.py â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ClickHouse RAW     â”‚
â”‚  (raw.raw_*)        â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  dbt Transformationsâ”‚
â”‚  (models/)          â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ClickHouse Analytics â”‚
â”‚ (analytics.*)       â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Streamlit Dashboard â”‚
â”‚ (streamlit_app/)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ’» Stack TecnolÃ³gico

| Componente | TecnologÃ­a | PropÃ³sito |
|------------|-----------|-----------|
| **Base de datos OLAP** | ClickHouse | Almacenamiento columnar de alto rendimiento |
| **Transformaciones** | dbt (Data Build Tool) | Transformaciones SQL con tests y docs |
| **OrquestaciÃ³n** | Docker Compose | ContainerizaciÃ³n y despliegue |
| **VisualizaciÃ³n** | Streamlit | Dashboards interactivos |
| **AnÃ¡lisis exploratorio** | Pandas | EDA y generaciÃ³n de reportes |
| **Control de versiones** | Git/GitHub | Versionado de cÃ³digo |
| **DocumentaciÃ³n** | MkDocs Material | DocumentaciÃ³n tÃ©cnica |

---

## ğŸ“‚ Estructura del Repositorio

```
F1-analitica/
â”‚
â”œâ”€â”€ datasets/                    # Datos fuente (CSV de FÃ³rmula 1)
â”‚   â”œâ”€â”€ circuits.csv
â”‚   â”œâ”€â”€ races.csv
â”‚   â”œâ”€â”€ drivers.csv
â”‚   â”œâ”€â”€ constructors.csv
â”‚   â”œâ”€â”€ results.csv
â”‚   â””â”€â”€ ... (mÃ¡s tablas)
â”‚
â”œâ”€â”€ scripts/                     # Scripts de ingesta
â”‚   â”œâ”€â”€ load_csvs_to_clickhouse.py
â”‚   â”œâ”€â”€ ingest_csvs_with_docker.sh
â”‚   â”œâ”€â”€ eda_pandas.py
â”‚   â””â”€â”€ .env (configuraciÃ³n)
â”‚
â”œâ”€â”€ dbt_project/                 # Proyecto dbt
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/            # Capa de staging
â”‚   â”‚   â”‚   â””â”€â”€ stg_races.sql
â”‚   â”‚   â”œâ”€â”€ dim/                # Tablas de dimensiÃ³n
â”‚   â”‚   â”‚   â””â”€â”€ dim_fecha.sql
â”‚   â”‚   â”œâ”€â”€ fact/               # Tablas de hechos
â”‚   â”‚   â”‚   â””â”€â”€ fact_race_results.sql
â”‚   â”‚   â””â”€â”€ marts/              # Data marts
â”‚   â”œâ”€â”€ snapshots/              # SCD Type 2
â”‚   â”‚   â””â”€â”€ driver_constructors_snapshot.sql
â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â””â”€â”€ profiles.yml.example
â”‚
â”œâ”€â”€ streamlit_app/              # Dashboard interactivo
â”‚   â”œâ”€â”€ constructor_dashboard.py
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ docs/                       # DocumentaciÃ³n MkDocs
â”‚   â”œâ”€â”€ architecture.md
â”‚   â”œâ”€â”€ implementation.md
â”‚   â”œâ”€â”€ metodologia.md
â”‚   â”œâ”€â”€ ingestion.md
â”‚   â”œâ”€â”€ snapshots.md
â”‚   â”œâ”€â”€ use_case.md
â”‚   â””â”€â”€ adr/                    # Architecture Decision Records
â”‚
â”œâ”€â”€ docker-compose.yml          # OrquestaciÃ³n de servicios
â”œâ”€â”€ environment.yml             # Ambiente Conda
â”œâ”€â”€ mkdocs.yml                  # ConfiguraciÃ³n MkDocs
â””â”€â”€ README.md                   # DocumentaciÃ³n principal
```

---

## ğŸš€ CÃ³mo Ejecutar el Proyecto

### Prerrequisitos
- Docker y Docker Compose instalados
- Python 3.9+ (para desarrollo local)
- Conda (opcional)

### Pasos de InstalaciÃ³n

```bash
# 1. Clonar el repositorio
git clone https://github.com/hlrd93/F1-analitica.git
cd F1-analitica

# 2. Configurar variables de entorno
cat > scripts/.env << EOF
CLICKHOUSE_HOST=clickhouse
CLICKHOUSE_PORT=9000
CLICKHOUSE_USER=default
CLICKHOUSE_PASSWORD=
EOF

# 3. Levantar ClickHouse
docker-compose up -d clickhouse

# 4. Ingestar datos
python scripts/load_csvs_to_clickhouse.py

# 5. Ejecutar transformaciones dbt
docker-compose up dbt-runner

# 6. Lanzar dashboard
cd streamlit_app
streamlit run constructor_dashboard.py
```

---

## ğŸ“Š Componentes Implementados

### 1. Capa de Ingesta (Raw Layer)
- **Script:** `scripts/load_csvs_to_clickhouse.py`
- **FunciÃ³n:** Carga CSVs a ClickHouse en schema `raw`
- **CaracterÃ­sticas:**
  - Lectura automÃ¡tica de esquema desde CSVs
  - Manejo de valores nulos (`\\N`)
  - Carga por lotes (BATCH_SIZE=10000)
  - CreaciÃ³n automÃ¡tica de bases de datos

### 2. Capa de TransformaciÃ³n (Analytics Layer)
- **Herramienta:** dbt
- **Modelos implementados:**
  - `staging/stg_races.sql` - NormalizaciÃ³n de carreras
  - `dim/dim_fecha.sql` - DimensiÃ³n temporal
  - `fact/fact_race_results.sql` - Hechos de resultados
- **CaracterÃ­sticas:**
  - Tests de calidad de datos
  - DocumentaciÃ³n en `schema.yml`
  - Snapshots SCD Type 2

### 3. Capa de VisualizaciÃ³n
- **Dashboard:** `streamlit_app/constructor_dashboard.py`
- **Funcionalidad:** AnÃ¡lisis interactivo de constructores
- **CaracterÃ­sticas:**
  - Filtros dinÃ¡micos
  - GrÃ¡ficos interactivos
  - ConexiÃ³n directa a ClickHouse Analytics

### 4. AnÃ¡lisis Exploratorio
- **Script:** `scripts/eda_pandas.py`
- **Output:** `datasets/eda_report.md`
- **Incluye:**
  - EstadÃ­sticas descriptivas por tabla
  - DetecciÃ³n de valores faltantes
  - AnÃ¡lisis de unicidad
  - Distribuciones de variables

---

## âœ… Requisitos AcadÃ©micos Cumplidos

| Requisito | Estado | Evidencia |
|-----------|--------|-----------|
| Modelado dimensional (Star Schema) | âœ… | `dbt_project/models/dim/` y `fact/` |
| Procesos ETL/ELT | âœ… | `scripts/load_csvs_to_clickhouse.py` + dbt |
| Calidad de datos | âœ… | Tests dbt en `schema.yml` |
| Dashboards BI | âœ… | `streamlit_app/constructor_dashboard.py` |
| DocumentaciÃ³n completa | âœ… | `docs/` + `README.md` |
| SCD Type 2 | âœ… | `dbt_project/snapshots/` |
| ContainerizaciÃ³n | âœ… | `docker-compose.yml` + Dockerfiles |

---

## ğŸ“– DocumentaciÃ³n Disponible

### En el Repositorio
1. **README.md** - GuÃ­a principal del proyecto
2. **docs/architecture.md** - Arquitectura del pipeline
3. **docs/implementation.md** - Referencias al cÃ³digo
4. **docs/metodologia.md** - Enfoque metodolÃ³gico
5. **docs/ingestion.md** - Proceso de ingesta
6. **docs/snapshots.md** - SCD Type 2
7. **docs/use_case.md** - Casos de uso
8. **docs/adr/** - Decisiones de arquitectura

### Generada
- **datasets/eda_report.md** - Reporte de anÃ¡lisis exploratorio
- **dbt_project/target/** - DocumentaciÃ³n dbt compilada

---

## ğŸ” Puntos Destacados del Proyecto

### Modelado Dimensional
- ImplementaciÃ³n de **Star Schema** con tablas de dimensiÃ³n y hechos
- DimensiÃ³n temporal (`dim_fecha`) con granularidad diaria
- Snapshots para rastrear cambios histÃ³ricos (SCD Type 2)

### Calidad de Datos
- Tests automatizados con dbt:
  - Unicidad de claves primarias
  - Integridad referencial
  - Valores no nulos en campos crÃ­ticos
  - Valores aceptados en campos categÃ³ricos

### Performance
- ClickHouse como motor OLAP columnar
- OptimizaciÃ³n de consultas con Ã­ndices
- Carga por lotes en la ingesta

### Buenas PrÃ¡cticas
- CÃ³digo versionado en Git
- ContainerizaciÃ³n con Docker
- DocumentaciÃ³n como cÃ³digo (MkDocs)
- SeparaciÃ³n de ambientes (raw/analytics)
- Variables de entorno para configuraciÃ³n

---

## ğŸ“ Contexto AcadÃ©mico

**Universidad:** Universidad CatÃ³lica del Uruguay (UCU)  
**Programa:** MaestrÃ­a en Ciencia de Datos  
**Materia:** GestiÃ³n y Gobernanza de Datos *(o materia correspondiente)*  
**PerÃ­odo:** 2025

### Objetivos de Aprendizaje Demostrados
1. DiseÃ±o e implementaciÃ³n de pipelines de datos
2. Modelado dimensional para anÃ¡lisis
3. Uso de herramientas modernas de ingenierÃ­a de datos
4. GestiÃ³n de calidad de datos
5. DocumentaciÃ³n tÃ©cnica profesional

---

## ğŸ“§ Contacto

**Estudiante:** Herwin  
**GitHub:** [@hlrd93](https://github.com/hlrd93)  
**Repositorio:** [F1-analitica](https://github.com/hlrd93/F1-analitica)

---

## ğŸ“ Notas para RevisiÃ³n

### Para Ejecutar el Proyecto
1. El repositorio es **pÃºblico** y accesible desde cualquier lugar
2. Todos los scripts y configuraciones estÃ¡n incluidos
3. Los datos estÃ¡n en el directorio `datasets/`
4. La documentaciÃ³n estÃ¡ completa en `docs/` y `README.md`

### Para EvaluaciÃ³n
- El cÃ³digo estÃ¡ organizado por capas (raw â†’ analytics)
- Cada componente tiene su directorio claramente identificado
- La documentaciÃ³n explica el propÃ³sito de cada archivo
- Los commits en Git muestran la evoluciÃ³n del proyecto

---

<p align="center">
  <strong>Proyecto desarrollado con dedicaciÃ³n para UCU ğŸ“</strong>
</p>
