{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#f1-analytics-documentacion-del-pipeline","title":"F1 Analytics \u2014 Documentaci\u00f3n del Pipeline","text":"<p>Bienvenido a la documentaci\u00f3n del proyecto <code>F1-analitica</code>.</p> <p>Este sitio recoge el dise\u00f1o, la implementaci\u00f3n y las decisiones tomadas durante la construcci\u00f3n del pipeline de datos que carga los CSVs con informaci\u00f3n de F1 en ClickHouse y genera modelos anal\u00edticos con <code>dbt</code>.</p> <p>Contenido relevante en el repo (referencias reales):</p> <ul> <li><code>datasets/</code> \u2014 CSVs de entrada (drivers, races, lap_times, etc.).</li> <li><code>scripts/ingest_csvs_with_docker.sh</code> \u2014 script reproducible que inserta CSVs en ClickHouse.</li> <li><code>scripts/load_csvs_to_clickhouse.py</code> \u2014 loader Python que usa <code>clickhouse-connect</code>.</li> <li><code>docker-compose.yml</code> \u2014 levanta ClickHouse y el contenedor <code>dbt-runner</code>.</li> <li><code>dbt_project/</code> \u2014 modelos y tests dbt para transformar datos desde <code>raw</code> hacia <code>analytics</code>.</li> <li><code>streamlit_app/</code> \u2014 aplicaci\u00f3n de visualizaci\u00f3n que consume los datos transformados.</li> </ul> <p>Usa la navegaci\u00f3n a la izquierda para explorar arquitectura, ingesti\u00f3n, implementaci\u00f3n y metodolog\u00edas.</p> <p>Si quieres que genere una exportaci\u00f3n HTML/PDF del sitio o que suba a GitHub Pages, dime y lo preparo.</p>"},{"location":"architecture/","title":"Arquitectura","text":""},{"location":"architecture/#arquitectura-del-pipeline-basada-en-el-codigo-del-repositorio","title":"Arquitectura del Pipeline (basada en el c\u00f3digo del repositorio)","text":"<p>Este diagrama refleja la arquitectura real implementada en el repo.</p>  flowchart LR   subgraph Source     A[CSV files]   end    subgraph Ingest     B[load_csvs]     C[ingest_shell]   end    subgraph RawDB     D[raw_tables]   end    subgraph Transform     E[dbt_models]   end    subgraph AnalyticsDB     F[analytics_tables]   end    subgraph Viz     G[streamlit]   end    A --&gt; B   A --&gt; C   B --&gt; D   C --&gt; D   D --&gt; E   E --&gt; F   F --&gt; G   H[docker_compose] -.-&gt; D   H -.-&gt; F  <p> </p> <p>Descripci\u00f3n de componentes (referencias reales):</p> <ul> <li>Origen: <code>datasets/</code> (CSV). Los archivos reales est\u00e1n en la carpeta <code>datasets/</code> del repo.</li> <li>Ingestor (staging): <code>scripts/load_csvs_to_clickhouse.py</code> y <code>scripts/ingest_csvs_with_docker.sh</code> \u2014 crean tablas <code>raw.raw_&lt;name&gt;</code> con todas las columnas como <code>String</code> y cargan datos inicialmente (ver <code>scripts/README.md</code>).</li> <li>Warehouse/Transform: <code>dbt_project/</code> \u2014 contiene modelos que consumen los <code>raw</code> tables y generan tablas en la base <code>analytics</code>.</li> <li>Visualizaci\u00f3n: <code>streamlit_app/</code> (ya presente en el repo) que lee desde la base anal\u00edtica.</li> <li>Orquestaci\u00f3n/despliegue local: <code>docker-compose.yml</code> levanta <code>clickhouse</code> y <code>dbt-runner</code> seg\u00fan el repositorio.</li> </ul> <p>Notas importantes observadas en el c\u00f3digo:</p> <ul> <li>La ingesti\u00f3n es intencionalmente de dos pasos: primero cargas en tablas <code>String</code> en <code>raw</code> para evitar errores de tipo, luego <code>dbt</code> hace las castings y validaciones.</li> <li>El contenedor <code>dbt-runner</code> en <code>docker-compose.yml</code> ejecuta <code>dbt run</code> y <code>dbt test</code> autom\u00e1ticamente en el ejemplo.</li> </ul>"},{"location":"implementation/","title":"Implementaci\u00f3n","text":""},{"location":"implementation/#implementacion-referencias-al-codigo-existente","title":"Implementaci\u00f3n \u2014 referencias al c\u00f3digo existente","text":"<p>Esta secci\u00f3n documenta los artefactos ya implementados en el repositorio.</p> <p>1) Scripts de ingesti\u00f3n</p> <ul> <li><code>scripts/ingest_csvs_with_docker.sh</code> \u2014 script shell que llama a <code>clickhouse-client</code> dentro del contenedor para crear tablas <code>raw.raw_&lt;name&gt;</code> y cargar CSVs.</li> <li><code>scripts/load_csvs_to_clickhouse.py</code> \u2014 loader Python con <code>clickhouse-connect</code>. Lee el encabezado para construir la tabla con todas las columnas como <code>String</code>, reemplaza <code>\\\\N</code> y hace inserts por lotes (BATCH_SIZE = 10000).</li> </ul> <p>2) Configuraci\u00f3n de despliegue local</p> <ul> <li><code>docker-compose.yml</code> \u2014 levanta servicio <code>clickhouse</code> y el contenedor <code>dbt-runner</code> (que ejecuta <code>dbt run</code> y <code>dbt test</code> en la configuraci\u00f3n del repo). El <code>docker-compose.yml</code> incluye un volumen <code>clickhouse_data</code>.</li> </ul> <p>3) dbt</p> <ul> <li><code>dbt_project/</code> \u2014 contiene <code>dbt_project.yml</code> y modelos. El perfil est\u00e1 configurado para <code>clickhouse</code> seg\u00fan <code>dbt_project/dbt_project.yml</code>.</li> </ul> <p>4) Visualizaci\u00f3n</p> <ul> <li><code>streamlit_app/</code> \u2014 aplicaci\u00f3n de ejemplo que lee los modelos anal\u00edticos y muestra dashboards interactivos.</li> </ul> <p>Notas operacionales extra\u00eddas del c\u00f3digo:</p> <ul> <li>Autenticaci\u00f3n: <code>scripts/.env</code> contiene variables para conectar con ClickHouse; <code>load_csvs_to_clickhouse.py</code> lee estas variables con <code>dotenv</code>.</li> <li>Bases de datos utilizadas: <code>raw</code> (staging) y <code>analytics</code> (resultados) \u2014 estas bases se crean en el loader Python si no existen.</li> </ul>"},{"location":"ingestion/","title":"Ingesti\u00f3n","text":""},{"location":"ingestion/#ingestion-pasos-reproducibles-basado-en-scripts","title":"Ingesti\u00f3n \u2014 pasos reproducibles (basado en <code>scripts/</code>)","text":"<p>El repositorio ya incluye dos maneras reproducibles de cargar los CSVs en ClickHouse:</p> <ul> <li>Enfoque por Docker (recomendado para evaluaci\u00f3n): <code>scripts/ingest_csvs_with_docker.sh</code>.</li> <li>Enfoque por Python: <code>scripts/load_csvs_to_clickhouse.py</code> (usa <code>clickhouse-connect</code>).</li> </ul> <p>Resumen de los pasos (tal como est\u00e1n en <code>scripts/README.md</code>):</p> <ol> <li>Levantar ClickHouse con <code>docker-compose.yml</code>:</li> </ol> <pre><code>docker compose --env-file ./scripts/.env up -d\n</code></pre> <ol> <li>Ejecutar el script de ingesti\u00f3n (el mismo loop usado en la carga original):</li> </ol> <pre><code>chmod +x scripts/ingest_csvs_with_docker.sh\n./scripts/ingest_csvs_with_docker.sh\n</code></pre> <ol> <li>Verificar conteos (ejemplo):</li> </ol> <pre><code>docker exec -i clickhouse clickhouse-client --query \"SELECT table, sum(rows) AS rows FROM system.parts WHERE database='raw' GROUP BY table ORDER BY table;\"\n</code></pre> <p>Decisiones observadas en el c\u00f3digo y su justificaci\u00f3n:</p> <ul> <li>Las tablas <code>raw.raw_&lt;name&gt;</code> se crean con todas las columnas como <code>String</code> para evitar problemas de parsing en la primera ingesti\u00f3n. Esto aparece en <code>scripts/load_csvs_to_clickhouse.py</code>.</li> <li>El loader Python reemplaza literal <code>\\\\N</code> por cadena vac\u00eda antes de insertar para evitar que ClickHouse lo interprete literalmente.</li> </ul> <p>Recomendaciones para la entrega:</p> <ul> <li>Mantener la copia exacta de <code>scripts/ingest_csvs_with_docker.sh</code> y el <code>docker-compose.yml</code> en la carpeta de entrega.</li> <li>Documentar en el informe por qu\u00e9 se hace la ingesti\u00f3n en <code>String</code> y c\u00f3mo <code>dbt</code> hace el casting posterior. Esto ya est\u00e1 indicado en <code>scripts/README.md</code>.</li> </ul>"},{"location":"metodologia/","title":"Metodolog\u00eda","text":""},{"location":"metodologia/#metodologia-scrum-plantilla-para-entrega-academica","title":"Metodolog\u00eda \u2014 Scrum (plantilla para entrega acad\u00e9mica)","text":"<p>Aunque la metodolog\u00eda no est\u00e1 codificada, incluyo aqu\u00ed la estructura de Scrum que puedes adaptar al informe de evaluaci\u00f3n.</p> <ul> <li>Roles: Product Owner (profesor/requirements), Scrum Master (t\u00fa), Equipo (estudiante/desarrollador).</li> <li>Ceremonias: planificaci\u00f3n de sprint, daily (breve), review y retrospective.</li> <li>Artefactos: Product Backlog, Sprint Backlog, Definition of Done.</li> </ul> <p>Propuesta pr\u00e1ctica para este proyecto (sugerida):</p> <ul> <li>3 sprints de 2 semanas cada uno:</li> <li>Sprint 1: Dise\u00f1o y puesta en marcha (docker, ingesti\u00f3n, diagramas, ADRs).</li> <li>Sprint 2: Limpieza y modelos (imputaci\u00f3n, dbt models, tests).</li> <li>Sprint 3: Documentaci\u00f3n final, visualizaci\u00f3n y entrega.</li> </ul> <p>EDT (WBS) m\u00ednima \u2014 ejemplo:</p> <ol> <li>Pipeline   1.1. Ingesti\u00f3n (scripts)   1.2. Staging (ClickHouse raw)   1.3. Transformaci\u00f3n (dbt)</li> <li>Documentaci\u00f3n</li> <li>Visualizaci\u00f3n</li> </ol> <p>Incluye un cronograma simple en la documentaci\u00f3n final (Gantt o tabla por sprint).</p>"},{"location":"use_case/","title":"Use case","text":""},{"location":"use_case/#caso-de-uso","title":"Caso de Uso","text":"<p>Objetivo principal</p> <p>Proveer un pipeline reproducible que transforme los CSVs originales en tablas anal\u00edticas hospedadas en ClickHouse para permitir an\u00e1lisis exploratorio y visualizaciones con <code>streamlit_app/</code>.</p> <p>Actores</p> <ul> <li>Data Engineer: prepara el entorno, ejecuta la ingesti\u00f3n y mantiene scripts.</li> <li>Analista: ejecuta dbt, valida los modelos y crea visualizaciones.</li> <li>Profesor/Reviewer: valida reproducibilidad y calidad de datos.</li> </ul> <p>Entradas</p> <ul> <li>CSVs en <code>datasets/</code> (drivers, races, lap_times, etc.).</li> </ul> <p>Salidas</p> <ul> <li>Tablas staging en la base <code>raw</code> de ClickHouse (<code>raw.raw_&lt;name&gt;</code>).</li> <li>Tablas transformadas en la base <code>analytics</code> (generadas por <code>dbt</code>).</li> <li>Documentaci\u00f3n en <code>docs/</code> y visualizaciones en <code>streamlit_app/</code>.</li> </ul> <p>Escenarios principales</p> <ol> <li>Flujo normal</li> <li>Levantar ClickHouse con <code>docker compose --env-file ./scripts/.env up -d</code>.</li> <li>Ejecutar <code>scripts/ingest_csvs_with_docker.sh</code> o <code>scripts/load_csvs_to_clickhouse.py</code>.</li> <li>Ejecutar <code>dbt run</code> (el repo incluye <code>dbt-runner</code> en <code>docker-compose.yml</code>).</li> <li> <p>Verificar tablas en <code>analytics</code> y abrir <code>streamlit_app/</code> para visualizaci\u00f3n.</p> </li> <li> <p>Reprocesado por calidad de datos</p> </li> <li> <p>Si <code>generate_datasets_profile.py</code> muestra columnas con nulos o tipos incorrectos,      editar scripts de transformaci\u00f3n en <code>dbt_project/</code> y re-run <code>dbt</code>.</p> </li> <li> <p>Entrega acad\u00e9mica</p> </li> <li>Generar site MkDocs con toda la documentaci\u00f3n y exportar a PDF/HTML para el profesor.</li> </ol> <p>Criterios de \u00e9xito</p> <ul> <li>Reproducibilidad: siguiendo las instrucciones, el evaluador puede levantar el entorno y   obtener las mismas tablas y conteos reportados en <code>scripts/README.md</code>.</li> <li>Calidad: modelos dbt pasan <code>dbt test</code> o se documentan claramente los tests fallidos.</li> <li>Documentaci\u00f3n: incluye arquitectura, metodolog\u00edas y ADRs para justificar decisiones.</li> </ul>"},{"location":"adr/0001-use-clickhouse/","title":"ADR 0001: Usar ClickHouse como almac\u00e9n anal\u00edtico","text":"<p>Status: Accepted</p> <p>Date: 2025-12-06</p> <p>Context:</p> <ul> <li>El repositorio incluye un servicio <code>clickhouse</code> definido en <code>docker-compose.yml</code>.</li> <li>Los datos de entrada son CSVs tabulares (datasets/). Se requieren consultas anal\u00edticas r\u00e1pidas para dashboards y agregaciones.</li> </ul> <p>Decision:</p> <ul> <li>Usar ClickHouse como almac\u00e9n anal\u00edtico principal. Se usa una base <code>raw</code> para ingesti\u00f3n inicial y una base <code>analytics</code> para los resultados transformados por <code>dbt</code>.</li> </ul> <p>Consecuencias:</p> <ul> <li>Ventajas: alto rendimiento en consultas anal\u00edticas, buenas capacidades de compresi\u00f3n y escalado para cargas de datos hist\u00f3ricas.</li> <li>Costes: ClickHouse requiere dise\u00f1o cuidadoso de <code>ORDER BY</code> y tipos para obtener m\u00e1ximo rendimiento; la ingesti\u00f3n inicial realiza staging con <code>String</code> para evitar parseos al vuelo.</li> </ul> <p>Alternativas consideradas:</p> <ul> <li>PostgreSQL (f\u00e1cil de usar, pero menos \u00f3ptimo para OLAP en grandes vol\u00famenes) \u2014 descartado por rendimiento en consultas anal\u00edticas.</li> </ul> <p>Referencias en el repo:</p> <ul> <li><code>docker-compose.yml</code> (servicio clickhouse)</li> <li><code>scripts/load_csvs_to_clickhouse.py</code> (crea bases <code>raw</code> y <code>analytics</code>, y carga CSVs)</li> </ul>"},{"location":"streamlit_extracts/","title":"Index","text":"<p>Tableau extracts and starter workbook</p> <p>Files in this folder:</p> <ul> <li><code>constructor_metrics_top200.csv</code> \u2014 sample/top-200 export used by the example workbook.</li> <li><code>constructor_time_series_sample.csv</code> \u2014 sample time-series rows used by the example workbook.</li> <li><code>constructor_metrics_full.csv</code> \u2014 full export of <code>analytics.constructor_metrics</code> from ClickHouse (no LIMIT).</li> <li><code>constructor_time_series_full.csv</code> \u2014 full export of <code>analytics.constructor_time_series</code> from ClickHouse (no LIMIT).</li> <li><code>constructor_workbook.twb</code> \u2014 a minimal Tableau workbook (XML) that references the CSV sample files above. Open it in Tableau Desktop and then save as <code>.twbx</code> if you want a packaged workbook.</li> </ul> <p>How to use</p> <ol> <li> <p>Open <code>constructor_workbook.twb</code> in Tableau Desktop. The workbook refers to the CSV files by relative path under <code>docs/tableau_extracts/</code> \u2014 if Tableau cannot find them, use File &gt; Open and point the datasource to the matching CSV location.</p> </li> <li> <p>If you want the full datasets, load <code>constructor_metrics_full.csv</code> and <code>constructor_time_series_full.csv</code> into Tableau (Data &gt; New Data Source &gt; Text File).</p> </li> <li> <p>If you modify the CSVs (for example after re-running dbt or ingesting cost data), refresh the data source in Tableau (Data &gt; Refresh) or re-point the connection to the updated CSV.</p> </li> </ol> <p>Ingesting real cost data (optional)</p> <p>If you have a CSV with constructor cost information (example name: <code>constructor_costs.csv</code> or <code>raw_constructor_costs.csv</code>) place it into the <code>datasets/</code> folder and run the ingestion script used for other CSVs:</p> <pre><code>./scripts/ingest_csvs_with_docker.sh\n</code></pre> <p>After ingesting, run the optional dbt model to compute real cost-normalized scores:</p> <pre><code>docker compose run --rm -v \"$PWD/profiles.compose.yml\":/workspace/profiles.yml --entrypoint /bin/bash dbt-runner -lc \"dbt run --project-dir /workspace/dbt_project --profiles-dir /workspace --select constructor_metrics_with_costs &amp;&amp; dbt test --project-dir /workspace/dbt_project --profiles-dir /workspace --select constructor_metrics_with_costs\"\n</code></pre> <p>Notes - The <code>.twb</code> included is minimal \u2014 you may want to replace the datasources in Tableau with the full CSVs or a live ClickHouse connection. - If you prefer I can (A) try to ingest a provided cost CSV and run the dbt model for you, or (B) produce an updated <code>.twb</code> that references the full CSVs. Tell me which and I'll proceed.</p>"}]}